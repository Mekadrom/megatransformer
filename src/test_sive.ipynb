{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bd4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pytorch bin\n",
      "Loaded model from ../../megatransformer/runs/gubert/tiny_deep_0_4/checkpoint-60000/pytorch_model.bin\n",
      "Loaded model from ../../megatransformer/runs/gubert/tiny_deep_0_4/checkpoint-60000/\n",
      "model  parameters: 7,037,822\n",
      "model  structure: SpeakerInvariantVoiceEncoder(\n",
      "  (conv_subsample): ConvSubsampling(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(80, 128, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "      (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout1d(p=0.1, inplace=False)\n",
      "      (4): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (5): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (6): GELU(approximate='none')\n",
      "      (7): Dropout1d(p=0.1, inplace=False)\n",
      "      (8): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (9): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      (10): GELU(approximate='none')\n",
      "      (11): Dropout1d(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_blocks): ModuleList(\n",
      "    (0-11): 12 x SpeakerInvariantVoiceEncoderBlock(\n",
      "      (rotary_embedding): RotaryEmbedding()\n",
      "      (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (head_dropout): Identity()\n",
      "      (conv_module): ConformerConvModule(\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (pointwise_conv1): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
      "        (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
      "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): SiLU()\n",
      "        (pointwise_conv2): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff1): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=1024, bias=True)\n",
      "        (1): SwiGLUSimple()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=1024, bias=True)\n",
      "        (1): SwiGLUSimple()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm_ff1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_ff2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_conv): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (feature_dropout): Identity()\n",
      "  (asr_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from model.audio.sive.sive import SpeakerInvariantVoiceEncoder\n",
    "\n",
    "\n",
    "def _load_model(\n",
    "    model_cls,\n",
    "    config_name: str,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "    device: str = \"cuda\",\n",
    "    overrides: dict = {},\n",
    "    strict: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a model from a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint directory (containing model.safetensors or pytorch_model.bin)\n",
    "        config_name: Config name from the model class (e.g., \"small\", \"medium\")\n",
    "        device: Device to load the model on\n",
    "\n",
    "    Returns:\n",
    "        model in eval mode\n",
    "    \"\"\"\n",
    "    # Create model with same config\n",
    "    model = model_cls.from_config(config_name, **overrides)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        return model\n",
    "\n",
    "    # Try to load from safetensors first, then pytorch_model.bin\n",
    "    safetensors_path = os.path.join(checkpoint_path, \"model.safetensors\")\n",
    "    pytorch_path = os.path.join(checkpoint_path, \"pytorch_model.bin\")\n",
    "    if os.path.exists(safetensors_path):\n",
    "        from safetensors.torch import load_file\n",
    "        state_dict = load_file(safetensors_path)\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "        print(f\"Loaded model from {safetensors_path}\")\n",
    "    elif os.path.exists(pytorch_path):\n",
    "        print(f\"loading pytorch bin\")\n",
    "        state_dict = torch.load(pytorch_path, map_location=device, weights_only=True)\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "        print(f\"Loaded model from {pytorch_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No model checkpoint found at {checkpoint_path}. \"\n",
    "            f\"Expected model.safetensors or pytorch_model.bin\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path, config):\n",
    "    if checkpoint_path is None:\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Model checkpoint not found at {checkpoint_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        model  = _load_model(SpeakerInvariantVoiceEncoder, config, checkpoint_path=checkpoint_path, strict=False)\n",
    "        model .eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model : {e}\")\n",
    "        raise e\n",
    "\n",
    "    print(f\"Loaded model from {checkpoint_path}\")\n",
    "    print(f\"model  parameters: {sum(p.numel() for p in model .parameters()):,}\")\n",
    "    print(f\"model  structure: {model }\")\n",
    "    return model \n",
    "    \n",
    "model  = load_model(\"../../megatransformer/runs/gubert/tiny_deep_0_4/checkpoint-60000/\", \"tiny_deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dbfb6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0372,  0.1138,  0.1117,  ...,  0.0039,  0.0294,  0.1461],\n",
       "        [ 0.0781, -0.0722, -0.0191,  ...,  0.1481,  0.0462, -0.1144],\n",
       "        [-0.0114,  0.0291, -0.0140,  ...,  0.0737, -0.1048,  0.1055],\n",
       "        ...,\n",
       "        [-0.1931, -0.0239, -0.1384,  ..., -0.0805, -0.0057, -0.1333],\n",
       "        [-0.0216,  0.0785,  0.1074,  ...,  0.0863, -0.0153,  0.0059],\n",
       "        [-0.0015,  0.0775, -0.0013,  ..., -0.0610,  0.0254,  0.0690]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder_blocks[-5].ff1[-2].weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
