from typing import Optional
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio


class ASRFeatureLoss(torch.nn.Module):
    def __init__(self, asr_model, sample_rate, feature_layer=-4, normalize=True, reduction='mean'):
        """
        ASR feature-based loss function for TTS training.
        
        Args:
            asr_model: Pre-trained ASR model (like Whisper) to extract features
            feature_layer: Which layer to extract features from (-1 is final, -2 is penultimate, etc.)
            normalize: Whether to normalize features before comparison
            reduction: How to reduce the loss ('mean', 'sum', or 'none')
        """
        super().__init__()
        self.asr_model = asr_model
        self.feature_layer = feature_layer
        self.normalize = normalize
        self.reduction = reduction
        
        # Freeze the ASR model
        for param in self.asr_model.parameters():
            param.requires_grad = False
        
        self.melspec_extractor = torchaudio.transforms.MelSpectrogram(n_mels=80, sample_rate=sample_rate)

        # Register hooks to get intermediate features
        self.features = {}
        self._register_hooks()
    
    def _register_hooks(self):
        """Register forward hooks to capture intermediate features."""
        def get_activation(name):
            def hook(model, input, output):
                self.features[name] = output
            return hook
        
        # Find the target layer
        target_modules = list(self.asr_model.modules())
        layer_idx = self.feature_layer
        if layer_idx < 0:
            layer_idx = len(target_modules) + layer_idx
            
        if 0 <= layer_idx < len(target_modules):
            target_modules[layer_idx].register_forward_hook(get_activation('target_features'))
    
    def _extract_mels(self, audio):
        return self.melspec_extractor(audio)

    def _get_features(self, audio):
        """Extract features from the ASR model."""
        self.features.clear()
        with torch.no_grad():
            # Forward pass without computing gradients
            mels = self._extract_mels(audio)
            # megatransformer_utils.print_debug_tensor('mels', mels)
            # pad to 3000 length
            if mels.shape[1] < 3000:
                mels = F.pad(mels, (0, 3000 - mels.shape[2]), mode='constant', value=0)
            elif mels.shape[1] > 3000:
                mels = mels[:, :3000]
            # megatransformer_utils.print_debug_tensor('padded_mels', mels)
            self.asr_model(mels)
            
        if 'target_features' not in self.features:
            raise ValueError(f"No features captured from layer {self.feature_layer}")
            
        features = self.features['target_features']
        
        # Handle different output types (tensor, tuple, etc.)
        if isinstance(features, tuple):
            features = features[0]  # Usually the first element is the main output
            
        if self.normalize and features.dim() >= 2:
            # Normalize along the feature dimension
            norm = torch.norm(features, dim=-1, keepdim=True)
            features = features / (norm + 1e-8)
            
        return features
    
    def forward(self, generated_audio, reference_audio):
        """
        Compute loss between generated and reference audio based on ASR features.
        
        Args:
            generated_audio: Audio generated by TTS model
            reference_audio: Reference/ground truth audio
            
        Returns:
            Loss value
        """
        # Extract features
        gen_features = self._get_features(generated_audio)
        ref_features = self._get_features(reference_audio)
        
        # Compute distance between features
        if gen_features.shape != ref_features.shape:
            # Handle shape mismatch (could happen with different audio lengths)
            min_time = min(gen_features.shape[1], ref_features.shape[1])
            gen_features = gen_features[:, :min_time, ...]
            ref_features = ref_features[:, :min_time, ...]
        
        # Compute cosine similarity loss
        similarity = torch.cosine_similarity(gen_features, ref_features, dim=-1)
        loss = 1.0 - similarity  # 0 is perfect match, 2 is complete opposite
        
        # Apply reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss

# Multi-Resolution STFT Loss for better vocoder training
class MultiResolutionSTFTLoss(nn.Module):
    """
    Multi-resolution STFT loss for better audio quality.
    
    This loss computes the L1 loss between the STFT of the predicted and
    ground truth waveforms at multiple resolutions, which helps capture
    both fine and coarse time-frequency structures.
    """
    def __init__(
        self,
        fft_sizes: list[int] = [512, 1024, 2048],
        hop_sizes: list[int] = [128, 256, 512],
        win_lengths: list[int] = [512, 1024, 2048],
        window: str = "hann_window"
    ):
        super().__init__()
        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)
        
        self.fft_sizes = fft_sizes
        self.hop_sizes = hop_sizes
        self.win_lengths = win_lengths
        
        # Create window buffers
        self.register_buffer(
            "window",
            torch.hann_window(max(win_lengths)) if window == "hann_window" else
            torch.hamming_window(max(win_lengths))
        )
    
    def stft_magnitude(
        self, 
        x: torch.Tensor, 
        fft_size: int, 
        hop_size: int, 
        win_length: int
    ) -> torch.Tensor:
        """Calculate STFT magnitude."""

        # stft requires float32 input
        x_float32 = x.float()
        x_stft = torch.stft(
            x_float32.squeeze(1),
            fft_size,
            hop_size,
            win_length,
            self.window[:win_length].to(x_float32.dtype),
            return_complex=True
        )
        return torch.abs(x_stft).to(x.dtype)
    
    def complex_stft_loss(self, pred, target, fft_size, hop_size, win_length):
        pred_stft = torch.stft(
            pred.float(), fft_size, hop_size, win_length,
            self.window[:win_length],
            return_complex=True
        )
        target_stft = torch.stft(
            target.float(), fft_size, hop_size, win_length,
            self.window[:win_length],
            return_complex=True
        )
        
        return F.l1_loss(pred_stft.real, target_stft.real) + F.l1_loss(pred_stft.imag, target_stft.imag)

    def forward(
        self, 
        pred_waveform: torch.Tensor, 
        target_waveform: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Calculate multi-resolution STFT loss.
        
        Args:
            pred_waveform: [B, 1, T] Predicted waveform
            target_waveform: [B, 1, T] Target waveform
            
        Returns:
            Tuple of (sc_loss, mag_loss) - spectral convergence and magnitude losses
        """
        sc_loss = 0.0
        mag_loss = 0.0
        complex_stft_loss = 0.0
        
        for fft_size, hop_size, win_length in zip(
            self.fft_sizes, self.hop_sizes, self.win_lengths
        ):
            pred_mag = self.stft_magnitude(
                pred_waveform, fft_size, hop_size, win_length
            )
            target_mag = self.stft_magnitude(
                target_waveform, fft_size, hop_size, win_length
            )
            
            # Spectral convergence loss
            target_norm = torch.norm(target_mag, p="fro").clamp(min=0.1)
            sc_loss += torch.norm(target_mag - pred_mag, p="fro") / target_norm
            
            # Log magnitude loss
            log_pred_mag = torch.log(pred_mag.clamp(min=1e-5))
            log_target_mag = torch.log(target_mag.clamp(min=1e-5))
            mag_loss += F.l1_loss(log_pred_mag, log_target_mag)

            # Complex STFT loss
            complex_stft_loss += self.complex_stft_loss(
                pred_waveform.squeeze(1),
                target_waveform.squeeze(1),
                fft_size,
                hop_size,
                win_length
            )
        
        # Normalize by number of STFT resolutions
        sc_loss = sc_loss / len(self.fft_sizes)
        mag_loss = mag_loss / len(self.fft_sizes)
        complex_stft_loss = complex_stft_loss / len(self.fft_sizes)
        return sc_loss, mag_loss, complex_stft_loss


class AudioGenerationLoss(nn.Module):
    """
    Combined loss function for training both diffusion model and vocoder.
    """
    def __init__(
        self,
        diffusion_loss_weight: float = 1.0,
        mel_loss_weight: float = 10.0,
        waveform_loss_weight: float = 1.0,
        stft_loss_weight: float = 2.0
    ):
        super().__init__()
        self.diffusion_loss_weight = diffusion_loss_weight
        self.mel_loss_weight = mel_loss_weight
        self.waveform_loss_weight = waveform_loss_weight
        self.stft_loss_weight = stft_loss_weight
        
        self.stft_loss = MultiResolutionSTFTLoss()
    
    def forward(
        self,
        pred_noise: torch.Tensor,
        noise: torch.Tensor,
        pred_mel: Optional[torch.Tensor] = None,
        target_mel: Optional[torch.Tensor] = None,
        pred_waveform: Optional[torch.Tensor] = None,
        target_waveform: Optional[torch.Tensor] = None
    ) -> tuple[torch.Tensor, dict]:
        """
        Calculate the combined loss.
        
        Returns:
            Tuple of (total_loss, loss_components_dict)
        """
        losses = {}
        
        # Diffusion loss (noise prediction)
        diffusion_loss = F.mse_loss(pred_noise, noise)
        losses["diffusion"] = diffusion_loss
        
        total_loss = self.diffusion_loss_weight * diffusion_loss
        
        # Optional mel loss
        if pred_mel is not None and target_mel is not None:
            mel_loss = F.l1_loss(pred_mel, target_mel)
            losses["mel"] = mel_loss
            total_loss = total_loss + self.mel_loss_weight * mel_loss
        
        # Optional waveform and STFT losses
        if pred_waveform is not None and target_waveform is not None:
            # Direct waveform loss
            waveform_loss = F.l1_loss(pred_waveform, target_waveform)
            losses["waveform"] = waveform_loss
            total_loss = total_loss + self.waveform_loss_weight * waveform_loss
            
            # Multi-resolution STFT loss
            sc_loss, mag_loss, complex_stft_loss = self.stft_loss(pred_waveform, target_waveform)
            losses["sc"] = sc_loss
            losses["mag"] = mag_loss
            losses["complex_stft"] = complex_stft_loss
            total_loss = total_loss + self.stft_loss_weight * (sc_loss + mag_loss + complex_stft_loss)
        
        return total_loss, losses

class StableMelSpectrogramLoss(nn.Module):
    def __init__(self, sample_rate, n_fft, hop_length, n_mels):
        super().__init__()
        self.n_fft = n_fft
        self.hop_length = hop_length
        
        # Pre-compute mel filterbank (no grad needed)
        mel_fb = torchaudio.functional.melscale_fbanks(
            n_freqs=n_fft // 2 + 1,
            f_min=0.0,
            f_max=sample_rate / 2,
            n_mels=n_mels,
            sample_rate=sample_rate,
        )
        self.register_buffer('mel_fb', mel_fb)
        self.register_buffer('window', torch.hann_window(n_fft))
    
    def forward(self, pred_waveform, target_log_mel):
        orig_dtype = pred_waveform.dtype
        # STFT
        stft = torch.stft(
            pred_waveform.float(),
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            window=self.window,
            return_complex=True,
        )
        
        # Stable magnitude with epsilon INSIDE sqrt
        magnitude_sq = (stft.real.pow(2) + stft.imag.pow(2)).to(orig_dtype)
        magnitude = torch.sqrt(magnitude_sq + 1e-6)  # Epsilon inside sqrt!
        
        # Mel filterbank
        mel = torch.matmul(magnitude.transpose(-1, -2), self.mel_fb).transpose(-1, -2)

        # Log with clamp
        log_mel = torch.log(mel.clamp(min=1e-5))
        
        # Match lengths
        min_len = min(log_mel.shape[-1], target_log_mel.shape[-1])
        
        return F.l1_loss(log_mel[..., :min_len], target_log_mel[..., :min_len])


def discriminator_loss(
    disc_real_outputs: list[torch.Tensor],
    disc_fake_outputs: list[torch.Tensor],
) -> torch.Tensor:
    """
    Discriminator loss: real samples should be classified as 1, fake as 0.
    Uses least-squares GAN loss.
    """
    loss = 0.0
    for dr, df in zip(disc_real_outputs, disc_fake_outputs):
        r_loss = torch.mean((1 - dr) ** 2)
        f_loss = torch.mean(df ** 2)
        loss += r_loss + f_loss
    return loss


def generator_loss(disc_fake_outputs: list[torch.Tensor]) -> torch.Tensor:
    """
    Generator loss: fake samples should be classified as 1 (fool discriminator).
    Uses least-squares GAN loss.
    """
    loss = 0.0
    for df in disc_fake_outputs:
        loss += torch.mean((1 - df) ** 2)
    return loss


def feature_matching_loss(
    disc_real_features: list[list[torch.Tensor]],
    disc_fake_features: list[list[torch.Tensor]],
) -> torch.Tensor:
    """
    Feature matching loss with proper normalization.
    """
    loss = 0.0
    num_layers = 0
    
    for real_feats, fake_feats in zip(disc_real_features, disc_fake_features):
        for real_feat, fake_feat in zip(real_feats, fake_feats):
            # Normalize by feature magnitude to make loss scale-invariant
            loss += F.l1_loss(fake_feat, real_feat.detach()) / (real_feat.detach().abs().mean() + 1e-7)
            num_layers += 1
    
    # Average over layers
    return loss / num_layers if num_layers > 0 else loss


def compute_discriminator_losses(
    disc_outputs_real: dict[str, tuple[list[torch.Tensor], list[list[torch.Tensor]]]],
    disc_outputs_fake: dict[str, tuple[list[torch.Tensor], list[list[torch.Tensor]]]],
) -> dict[str, torch.Tensor]:
    """Compute discriminator losses for all discriminator types."""
    losses = {}
    
    for key in disc_outputs_real:
        real_outs, _ = disc_outputs_real[key]
        fake_outs, _ = disc_outputs_fake[key]
        loss = discriminator_loss(real_outs, fake_outs)
        losses[f"d_loss_{key}"] = loss
    
    return losses


def compute_generator_losses(
    disc_outputs_fake: dict[str, tuple[list[torch.Tensor], list[list[torch.Tensor]]]],
    disc_outputs_real: dict[str, tuple[list[torch.Tensor], list[list[torch.Tensor]]]],
    fm_weight: float = 2.0,
) -> dict[str, torch.Tensor]:
    """Compute generator adversarial and feature matching losses."""
    losses = {}
    
    for key in disc_outputs_fake:
        fake_outs, fake_feats = disc_outputs_fake[key]
        real_outs, real_feats = disc_outputs_real[key]
        
        adv_loss = generator_loss(fake_outs)
        fm_loss = feature_matching_loss(real_feats, fake_feats)
        
        losses[f"g_adv_{key}"] = adv_loss
        losses[f"g_fm_{key}"] = fm_loss
    
    return losses
