maxlen: 1024
d_model: 512
dropout: 0.1

tokenizer: bert-base-multilingual-cased

positional_encoding: rotary
positional_embedding_dim: 64

tie_embeddings: False
padding_value: 0

decoder_config:
  device: cuda:0
  self_attn_config:
    n_heads: 8
    n_gqa_groups: 1
    d_queries: 64
    d_values: 64
  cross_attn_config:
    n_heads: 8
    n_gqa_groups: 1
    d_queries: 64
    d_values: 64
  n_layers: 6
  vocab_size: 119547

ffn_config:
  ffn_type: phi3
  d_inner: 2048
  activation_function: swiglu
